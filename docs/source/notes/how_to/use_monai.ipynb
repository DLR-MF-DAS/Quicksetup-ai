{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642368c8",
   "metadata": {},
   "source": [
    "# MedNIST classification\n",
    "MedNIST classification using MONAI and MLPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfc256",
   "metadata": {},
   "source": [
    "## Setup the data\n",
    "- To setup the data, we define two classes:\n",
    "    - `ClassificationTaskDataModule` (inherit from `pytorch_lightning.LightningDataModule`)\n",
    "    - `MedNISTDataset` (inherits from `torch.utils.data.Dataset`)\n",
    "    - We saved it as `ML-Pipeline-Template/src/ml_pipeline_template/datamodules/classification.py`\n",
    "- Below are the contents of this file containing the methods we need to define.\n",
    "    - `prepare_data`\n",
    "        - download MedNIST tar file and saved it under `data/MedNIST/raw`\n",
    "        - extract contents of downloaded file and save it under `data/MedNIST/processed`\n",
    "    - `setup`\n",
    "        - define transforms\n",
    "        - split dataset into train|test|validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from monai.transforms import (\n",
    "    AddChannel,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandFlip,\n",
    "    EnsureType,\n",
    "    ScaleIntensity\n",
    ")\n",
    "\n",
    "from monai.apps import download_url, extractall\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from ml_pipeline_template import utils\n",
    "\n",
    "log = utils.get_logger(__name__)\n",
    "\n",
    "\n",
    "class ClassificationTaskDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, num_classes=6, batch_size: int = 64,\n",
    "                 num_workers: int = 0, pin_memory: bool = False):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.img_all = []\n",
    "        self.train_set = None\n",
    "        self.val_set = None\n",
    "        self.test_set = None\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.label_all = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download data and save under `data/raw/` directory.\n",
    "        data_dir_raw = os.path.join(self.data_dir, \"MedNIST/raw/\")\n",
    "        data_dir_processed = os.path.join(self.data_dir, \"MedNIST/processed/\")\n",
    "        resource = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\"\n",
    "        md5 = \"0bc7306e7427e00ad1c5526a6677552d\"\n",
    "        compressed_file = os.path.join(data_dir_raw, \"MedNIST.tar.gz\")\n",
    "        if not os.path.exists(compressed_file):\n",
    "            download_url(url=resource, filepath=compressed_file, hash_val=md5)\n",
    "        if not os.path.exists(data_dir_processed):\n",
    "            extractall(filepath=compressed_file, output_dir=data_dir_processed)\n",
    "        new_data_dir = os.path.join(data_dir_processed, \"**\", \"*.jpeg\")\n",
    "        self.img_all = np.array(glob.glob(new_data_dir, recursive=True))\n",
    "        log.info(f\"Total images: {len(self.img_all)}\")\n",
    "        self.label_all = np.array([i.split(\"/\")[-2] for i in self.img_all])\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        self.label_all = le.fit_transform(self.label_all)\n",
    "        log.info(f\"Total labels: {len(self.label_all)}\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Define transforms\n",
    "        train_transforms = Compose(\n",
    "            [LoadImage(image_only=True), AddChannel(), ScaleIntensity(),\n",
    "             RandFlip(spatial_axis=0, prob=0.5),\n",
    "             EnsureType()]\n",
    "        )\n",
    "        val_transforms = Compose(\n",
    "            [LoadImage(image_only=True), AddChannel(), ScaleIntensity(), EnsureType()]\n",
    "        )\n",
    "        # Split dataset in to train, val, test sets.\n",
    "        num_samples = len(self.img_all)\n",
    "        tr_split = int(num_samples * .8)\n",
    "        val_split = int(num_samples * .9)\n",
    "        all_indices = np.arange(num_samples)\n",
    "        np.random.shuffle(all_indices)\n",
    "        train_indices = all_indices[:tr_split]\n",
    "        val_indices = all_indices[tr_split:val_split]\n",
    "        test_indices = all_indices[val_split:]\n",
    "        train_subjects, train_labels = self.img_all[train_indices], self.label_all[\n",
    "            train_indices]\n",
    "        val_subjects, val_labels = self.img_all[val_indices], self.label_all[\n",
    "            val_indices]\n",
    "        test_subjects, test_labels = self.img_all[test_indices], self.label_all[\n",
    "            test_indices]\n",
    "\n",
    "        self.train_set = MedNISTDataset(train_subjects, train_labels, train_transforms)\n",
    "        self.val_set = MedNISTDataset(val_subjects, val_labels, val_transforms)\n",
    "        self.test_set = MedNISTDataset(test_subjects, test_labels, val_transforms)\n",
    "\n",
    "        log.info(f\"Number of trainset: {len(self.train_set)}\")\n",
    "        log.info(f\"Number of testset: {len(self.test_set)}\")\n",
    "        log.info(f\"Number of valset: {len(self.val_set)}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers,\n",
    "                          drop_last=True, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers, drop_last=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers, drop_last=True)\n",
    "\n",
    "\n",
    "class MedNISTDataset(Dataset):\n",
    "    def __init__(self, img_paths, label_paths, transforms):\n",
    "        self.img_paths = img_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transforms(self.img_paths[index]), self.label_paths[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659fb1d9",
   "metadata": {},
   "source": [
    "## Setup the config for data\n",
    "- This `yaml` file will be used to initialize `ClassificationTaskDataModule`.\n",
    "- We create a `yaml` file under `.ML-Pipeline-Template/configs/datamodule/` and named it `mednist.yaml`\n",
    "- Below are the contents of the corresponding `yaml` file for the custom `ClassificationTaskDataModule`.\n",
    "    - File saved in `.ML-Pipeline-Template/configs/datamodule/mednist.yaml`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ca3cee8",
   "metadata": {},
   "source": [
    "_target_: ml_pipeline_template.datamodules.classification.ClassificationTaskDataModule\n",
    "\n",
    "# Any argument in the `ClassificationTaskDataModule` can be modified using this file\n",
    "data_dir: ${data_dir}\n",
    "num_classes: 6\n",
    "batch_size: 256\n",
    "num_workers: 0\n",
    "pin_memory: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc7f619",
   "metadata": {},
   "source": [
    "## Setup the model\n",
    "- We will use the `DenseNet169` model from MONAI.\n",
    "    - First, write your own class which inherits from `pytorch_lightning.LightningModule`\n",
    "    - Save it under `ML-Pipeline-Template/src/ml_pipeline_template/models/`.\n",
    "    - Below are the contents of the example file (`ML-Pipeline-Template/src/ml_pipeline_template/models/classification.py`) we will use in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "\n",
    "from ml_pipeline_template import utils\n",
    "\n",
    "log = utils.get_logger(__name__)\n",
    "\n",
    "\n",
    "class ClassificationTaskModule(LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            net: torch.nn.Module,\n",
    "            lr: float = 0.001,\n",
    "            weight_decay: float = 0.0005,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.train_acc = Accuracy()\n",
    "        self.val_acc = Accuracy()\n",
    "        self.test_acc = Accuracy()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "    def step(self, batch: Any):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return loss, preds, y\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.step(batch)\n",
    "        acc = self.train_acc(preds, targets)\n",
    "        self.log(\"train/loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.step(batch)\n",
    "        acc = self.val_acc(preds, targets)\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"val/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.step(batch)\n",
    "        acc = self.test_acc(preds, targets)\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test/acc\", acc, on_step=False, on_epoch=True)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            params=self.parameters(), lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e72638",
   "metadata": {},
   "source": [
    "## Setup the config for model\n",
    "- We create a `yaml` file named `mednist.yaml` inside `.ML-Pipeline-Template/configs/datamodule/`\n",
    "- Below are the contents of the corresponding `yaml` file for `ClassificationTaskModule`.\n",
    "    - Saved in `ML-Pipeline-Template/configs/datamodule/densenet.yaml`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cceaed90",
   "metadata": {},
   "source": [
    "_target_: ml_pipeline_template.models.custom_model.MedNISTModel\n",
    "    \n",
    "lr: 0.001\n",
    "weight_decay: 0.0005\n",
    "net:\n",
    "  _target_: monai.networks.nets.DenseNet169\n",
    "  spatial_dims: 2 \n",
    "  in_channels: 1\n",
    "  out_channels: 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421657c",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "CPU-based:\n",
    "```bash\n",
    "python scripts/train.py datamodule=mednist model=densenet logger=tensorboard\n",
    "```\n",
    "\n",
    "GPU-based:\n",
    "```bash\n",
    "python scripts/train.py datamodule=mednist model=densenet trainer.gpus=1 logger=tensorboard\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c31db",
   "metadata": {},
   "source": [
    "## Model testing\n",
    "The best model is saved as logs/experiments/runs/default/{date}/checkpoints/epoch_{num}.ckpt\n",
    "\n",
    "    \n",
    "CPU-based:\n",
    "```bash\n",
    "CKPT_PATH=logs/experiments/runs/default/2022-05-03_18-52-31/checkpoints/epoch_004.ckpt\n",
    "python scripts/test.py datamodule=mednist model=densenet ckpt_path=$CKPT_PATH\n",
    "```\n",
    "\n",
    "GPU-based:\n",
    "```bash\n",
    "CKPT_PATH=logs/experiments/runs/default/2022-05-03_18-52-31/checkpoints/epoch_004.ckpt\n",
    "python scripts/test.py datamodule=mednist model=densenet trainer.gpus=1 ckpt_path=$CKPT_PATH\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}